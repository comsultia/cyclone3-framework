#!/bin/perl
package CRON::module;
use strict;

=head1 NAME

110-sitemap.0.cron

=cut

=head1 DESCRIPTION

This cron module automatically generates google sitemap xml index file. File is stored in !domain.tld/!www/sitemap.xml file.

=cut

=head1 DEPENDS

=over

=item *

Time::Local

=item *

XML::Generator

=item *

L<TOM::lock|source-doc/".core/.libs/TOM/lock.pm">

=back

=cut

=head1 SYNOPSIS

Informations is stored in service type files.

 <CONF_VAR id="sitemap" value="1" />
 <CONF_VAR id="weight" value="1" />
 <CONF_VAR id="lastmod" value="2006-12-12" />
 <CONF_VAR id="changefreq" value="weekly" />

Or you can setup it from modules:

 $main::sitemap=1;
 $main::env{'changefreq'}="weekly";
 $main::env{'lastmod'}="2006-12-12";
 $main::env{'weight'}="1";

Automatized calculation of weight in sitemap.xml generation is currently not very well.

=cut

use Time::Local;
use XML::Generator;
use TOM::lock;

sub execute
{
	alarm(3600);
	my %env=@_;
	
	TOM::Database::connect::multi('stats') || die "cannot connect all databases";
	
	my $lock=new TOM::lock("sitemap generating") || return 1;
	
	my $from=$main::time_current-(86400*31);
	
	my $sql=qq{
		SELECT
			requests AS weight,
			url
		FROM
			TOM.a110_sitemap
		WHERE
			domain='$tom::Hm' AND
			domain_sub='$tom::H' AND
			url LIKE '$tom::H_www%' AND
			weight=0
		ORDER BY requests DESC
		LIMIT 1
	};
	main::_log("sql:=$sql");
	my $db0=$main::DB{'stats'}->Query($sql);
	my %db0_line=$db0->fetchhash();
	my $weight_full=$db0_line{'weight'};
	
	main::_log("max weight is $weight_full on url='$db0_line{url}'");
	
	return 1 unless $weight_full;
	
	my $X = XML::Generator->new(':pretty');
	
	my $file=$tom::P.'/!www/sitemap.xml';
	main::_log("writing to file $file");
	open(ZIP,'>'.$file) || die "$!";
	print ZIP "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n";
	print ZIP "<urlset xmlns=\"http://www.google.com/schemas/sitemap/0.84\">\n";
	
	my $sql=qq{
		SELECT
			*
		FROM
			TOM.a110_sitemap
		WHERE
			domain='$tom::Hm' AND
			domain_sub='$tom::H' AND
			time_use>=$from AND
			url LIKE '$tom::H_www%' AND
			requests>=$weight_full/1000
		ORDER BY requests DESC
		LIMIT 50000
	};
	main::_log("sql:=$sql");
	my $db0=$main::DB{'stats'}->Query($sql);
	my $i;
	while (my %db0_line=$db0->fetchhash())
	{
		next if $db0_line{'url'} eq "http://";
		
		$i++;
		
		$main::DB{'stats'}->Query("
			UPDATE TOM.a110_sitemap SET time_generate='$main::time_current' WHERE ID='$db0_line{ID}' LIMIT 1
		");
		#my $priority=$db0_line{'requests'}/$weight_full;
		my $priority=$db0_line{'weight'};
		$priority=$db0_line{'requests'}/$weight_full unless $priority;
		
		$priority=sprintf("%.1f",$priority);
		$priority="0.1" if $priority eq "0.0";
		
		my $changefreq=$db0_line{'changefreq'};
		$changefreq="weekly" unless $changefreq;
		
		my $lastmod=$db0_line{'lastmod'};
		$lastmod=$main::Fyear.'-'.$main::Fmom.'-'.$main::Fmday unless $lastmod;
		
		main::_log("[$i] add URL='$db0_line{'url'}' priority='$priority' changefreq='$changefreq' lastmod='$lastmod'");
		
		print ZIP $X->url(
			$X->loc($db0_line{'url'}),
			$X->lastmod($lastmod),
			$X->priority($priority),
			$X->changefreq($changefreq)
		)."\n";
	}
	
	print ZIP "</urlset>\n";
	close(ZIP);
	
	$lock->close();
	
	return 1
}

1;
